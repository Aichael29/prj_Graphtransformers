{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "from dgl.nn.functional import edge_softmax\n",
    "from dgl.nn import HeteroGraphConv, GATConv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the graph from the .bin file\n",
    "graph = dgl.load_graphs('C:/Users/suman/OneDrive/Bureau/Case_Study/prj_Graphtransformers/prj_Graphtransformers/src/data/processed/oulad_graph_with_features.bin')[0][0]\n",
    "# graph = graphs[0].to(device)\n",
    "\n",
    "# Check the graph structure\n",
    "print(\"Graph info:\")\n",
    "print(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming student nodes have a unique 'student_id' feature\n",
    "if 'id_student' in graph.nodes['student'].data:\n",
    "    graph_student_ids = graph.nodes['student'].data['id_student'].numpy()\n",
    "    print(\"Sample student IDs from the graph:\", graph_student_ids[:10])\n",
    "else:\n",
    "    print(\"No 'student_id' found in graph! Ensure the graph contains student IDs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load student labels from studentInfo.csv\n",
    "student_info = pd.read_csv(\"C:/Users/suman/OneDrive/Bureau/Case_Study/prj_Graphtransformers/prj_Graphtransformers/data/raw/studentInfo.csv\")\n",
    "\n",
    "# Convert 'final_result' into numerical labels\n",
    "result_mapping = {\"Withdrawn\": 0, \"Fail\": 1, \"Pass\": 2, \"Distinction\": 2}\n",
    "student_info[\"labels\"] = student_info[\"final_result\"].map(result_mapping)\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels_tensor = torch.tensor(student_info[\"labels\"].values, dtype=torch.long)\n",
    "\n",
    "# Ensure label tensor matches student node count\n",
    "if labels_tensor.shape[0] != graph.num_nodes('student'):\n",
    "    print(\"⚠️ Mismatch between student nodes and labels! Check data order.\")\n",
    "else:\n",
    "    # Assign labels to student nodes in the graph\n",
    "    graph.nodes['student'].data['labels'] = labels_tensor\n",
    "    print(\"✅ Labels successfully added to student nodes!\")\n",
    "\n",
    "# Verify update\n",
    "print(\"Updated keys for student nodes:\", graph.nodes['student'].data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HGTLayer: Responsible for performing multi-head attention over different node types and edges in your heterogeneous graph\n",
    "\n",
    "class HGTLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        node_dict,\n",
    "        edge_dict,\n",
    "        n_heads,\n",
    "        dropout=0.2,\n",
    "        use_norm=False,\n",
    "    ):\n",
    "        super(HGTLayer, self).__init__()\n",
    "\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.node_dict = node_dict\n",
    "        self.edge_dict = edge_dict\n",
    "        self.num_types = len(node_dict)\n",
    "        self.num_relations = len(edge_dict)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = out_dim // n_heads\n",
    "        self.sqrt_dk = math.sqrt(self.d_k)\n",
    "\n",
    "        # Linear transformations for query, key, value\n",
    "        self.k_linears = nn.ModuleList()\n",
    "        self.q_linears = nn.ModuleList()\n",
    "        self.v_linears = nn.ModuleList()\n",
    "        self.a_linears = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.use_norm = use_norm\n",
    "\n",
    "        for _ in range(self.num_types):\n",
    "            self.k_linears.append(nn.Linear(in_dim, out_dim))\n",
    "            self.q_linears.append(nn.Linear(in_dim, out_dim))\n",
    "            self.v_linears.append(nn.Linear(in_dim, out_dim))\n",
    "            self.a_linears.append(nn.Linear(out_dim, out_dim))\n",
    "            if use_norm:\n",
    "                self.norms.append(nn.LayerNorm(out_dim))\n",
    "\n",
    "        # Relation-specific attention and message passing parameters\n",
    "        self.relation_pri = nn.Parameter(torch.ones(self.num_relations, n_heads))\n",
    "        self.relation_att = nn.Parameter(torch.Tensor(self.num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.relation_msg = nn.Parameter(torch.Tensor(self.num_relations, n_heads, self.d_k, self.d_k))\n",
    "        self.skip = nn.Parameter(torch.ones(self.num_types))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize parameters\n",
    "        nn.init.xavier_uniform_(self.relation_att)\n",
    "        nn.init.xavier_uniform_(self.relation_msg)\n",
    "\n",
    "    def forward(self, G, h):\n",
    "        with G.local_scope():\n",
    "            node_dict, edge_dict = self.node_dict, self.edge_dict\n",
    "            for srctype, etype, dsttype in G.canonical_etypes:\n",
    "                sub_graph = G[srctype, etype, dsttype]\n",
    "\n",
    "                k_linear = self.k_linears[node_dict[srctype]]\n",
    "                v_linear = self.v_linears[node_dict[srctype]]\n",
    "                q_linear = self.q_linears[node_dict[dsttype]]\n",
    "\n",
    "                k = k_linear(h[srctype]).view(-1, self.n_heads, self.d_k)\n",
    "                v = v_linear(h[srctype]).view(-1, self.n_heads, self.d_k)\n",
    "                q = q_linear(h[dsttype]).view(-1, self.n_heads, self.d_k)\n",
    "\n",
    "                e_id = edge_dict[etype]\n",
    "\n",
    "                relation_att = self.relation_att[e_id]\n",
    "                relation_pri = self.relation_pri[e_id]\n",
    "                relation_msg = self.relation_msg[e_id]\n",
    "\n",
    "                k = torch.einsum(\"bij,ijk->bik\", k, relation_att)\n",
    "                v = torch.einsum(\"bij,ijk->bik\", v, relation_msg)\n",
    "\n",
    "                sub_graph.srcdata[\"k\"] = k\n",
    "                sub_graph.dstdata[\"q\"] = q\n",
    "                sub_graph.srcdata[\"v_%d\" % e_id] = v\n",
    "\n",
    "                sub_graph.apply_edges(fn.v_dot_u(\"q\", \"k\", \"t\"))\n",
    "                attn_score = (sub_graph.edata.pop(\"t\").sum(-1) * relation_pri) / self.sqrt_dk\n",
    "                attn_score = edge_softmax(sub_graph, attn_score, norm_by=\"dst\")\n",
    "\n",
    "                sub_graph.edata[\"t\"] = attn_score.unsqueeze(-1)\n",
    "\n",
    "            G.multi_update_all(\n",
    "                {\n",
    "                    etype: (\n",
    "                        fn.u_mul_e(\"v_%d\" % e_id, \"t\", \"m\"),\n",
    "                        fn.sum(\"m\", \"t\"),\n",
    "                    )\n",
    "                    for etype in edge_dict\n",
    "                },\n",
    "                cross_reducer=\"mean\",\n",
    "            )\n",
    "\n",
    "            new_h = {}\n",
    "            for ntype in G.ntypes:\n",
    "                n_id = node_dict[ntype]\n",
    "                alpha = torch.sigmoid(self.skip[n_id])\n",
    "                t = G.nodes[ntype].data[\"t\"].view(-1, self.out_dim)\n",
    "                trans_out = self.drop(self.a_linears[n_id](t))\n",
    "                trans_out = trans_out * alpha + h[ntype] * (1 - alpha)\n",
    "                if self.use_norm:\n",
    "                    new_h[ntype] = self.norms[n_id](trans_out)\n",
    "                else:\n",
    "                    new_h[ntype] = trans_out\n",
    "            return new_h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define HGT model\n",
    "class HeteroGraphTransformer(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, num_heads, node_dict, edge_dict, dropout=0.2):\n",
    "        super(HeteroGraphTransformer, self).__init__()\n",
    "\n",
    "        self.node_dict = node_dict\n",
    "        self.edge_dict = edge_dict\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.gat_layers.append(\n",
    "                HGTLayer(\n",
    "                    hidden_dim,\n",
    "                    hidden_dim,\n",
    "                    node_dict,\n",
    "                    edge_dict,\n",
    "                    num_heads,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Final MLP for classification\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(64, 4)  # Predicts 3 classes: dropout, fail, pass\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, graph):\n",
    "        h = {ntype: graph.nodes[ntype].data['inp'] for ntype in graph.ntypes}\n",
    "\n",
    "        for layer in self.gat_layers:\n",
    "            h = layer(graph, h)\n",
    "\n",
    "        # Final classification on student nodes\n",
    "        out = self.fc1(h['student'])\n",
    "        out = self.dropout(F.relu(out))\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
