{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dgl.nn import GraphConv\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assessments Data:   code_module code_presentation  id_assessment assessment_type   date  weight\n",
      "0         AAA             2013J           1752             TMA   19.0    10.0\n",
      "1         AAA             2013J           1753             TMA   54.0    20.0\n",
      "2         AAA             2013J           1754             TMA  117.0    20.0\n",
      "3         AAA             2013J           1755             TMA  166.0    20.0\n",
      "4         AAA             2013J           1756             TMA  215.0    30.0\n",
      "Courses Data:   code_module code_presentation  module_presentation_length\n",
      "0         AAA             2013J                         268\n",
      "1         AAA             2014J                         269\n",
      "2         BBB             2013J                         268\n",
      "3         BBB             2014J                         262\n",
      "4         BBB             2013B                         240\n",
      "Student Assessment Data:    id_assessment  id_student  date_submitted  is_banked  score\n",
      "0           1752       11391              18          0   78.0\n",
      "1           1752       28400              22          0   70.0\n",
      "2           1752       31604              17          0   72.0\n",
      "3           1752       32885              26          0   69.0\n",
      "4           1752       38053              19          0   79.0\n",
      "Student Info Data:   code_module code_presentation  id_student gender                region  \\\n",
      "0         AAA             2013J       11391      M   East Anglian Region   \n",
      "1         AAA             2013J       28400      F              Scotland   \n",
      "2         AAA             2013J       30268      F  North Western Region   \n",
      "3         AAA             2013J       31604      F     South East Region   \n",
      "4         AAA             2013J       32885      F  West Midlands Region   \n",
      "\n",
      "       highest_education imd_band age_band  num_of_prev_attempts  \\\n",
      "0       HE Qualification  90-100%     55<=                     0   \n",
      "1       HE Qualification   20-30%    35-55                     0   \n",
      "2  A Level or Equivalent   30-40%    35-55                     0   \n",
      "3  A Level or Equivalent   50-60%    35-55                     0   \n",
      "4     Lower Than A Level   50-60%     0-35                     0   \n",
      "\n",
      "   studied_credits disability final_result  \n",
      "0              240          N         Pass  \n",
      "1               60          N         Pass  \n",
      "2               60          Y    Withdrawn  \n",
      "3               60          N         Pass  \n",
      "4               60          N         Pass  \n",
      "Student Registration Data:   code_module code_presentation  id_student  date_registration  \\\n",
      "0         AAA             2013J       11391             -159.0   \n",
      "1         AAA             2013J       28400              -53.0   \n",
      "2         AAA             2013J       30268              -92.0   \n",
      "3         AAA             2013J       31604              -52.0   \n",
      "4         AAA             2013J       32885             -176.0   \n",
      "\n",
      "   date_unregistration  \n",
      "0                  NaN  \n",
      "1                  NaN  \n",
      "2                 12.0  \n",
      "3                  NaN  \n",
      "4                  NaN  \n",
      "Student VLE Data:   code_module code_presentation  id_student  id_site  date  sum_click\n",
      "0         AAA             2013J       28400   546652   -10          4\n",
      "1         AAA             2013J       28400   546652   -10          1\n",
      "2         AAA             2013J       28400   546652   -10          1\n",
      "3         AAA             2013J       28400   546614   -10         11\n",
      "4         AAA             2013J       28400   546714   -10          1\n",
      "VLE Data:    id_site code_module code_presentation activity_type  week_from  week_to\n",
      "0   546943         AAA             2013J      resource        NaN      NaN\n",
      "1   546712         AAA             2013J     oucontent        NaN      NaN\n",
      "2   546998         AAA             2013J      resource        NaN      NaN\n",
      "3   546888         AAA             2013J           url        NaN      NaN\n",
      "4   547035         AAA             2013J      resource        NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "# Define paths to the datasets\n",
    "assessments_path = 'C:/Users/dell latitude 7400/Documents/case_study/prj_Graphtransformers/data/raw/assessments.csv'\n",
    "courses_path = 'C:/Users/dell latitude 7400/Documents/case_study/prj_Graphtransformers/data/raw/courses.csv'\n",
    "student_assessment_path = 'C:/Users/dell latitude 7400/Documents/case_study/prj_Graphtransformers/data/raw/studentAssessment.csv'\n",
    "student_info_path = 'C:/Users/dell latitude 7400/Documents/case_study/prj_Graphtransformers/data/raw/studentInfo.csv'\n",
    "student_registration_path = 'C:/Users/dell latitude 7400/Documents/case_study/prj_Graphtransformers/data/raw/studentRegistration.csv'\n",
    "student_vle_path = 'C:/Users/dell latitude 7400/Documents/case_study/prj_Graphtransformers/data/raw/studentVle.csv'\n",
    "vle_path = 'C:/Users/dell latitude 7400/Documents/case_study/prj_Graphtransformers/data/raw/vle.csv'\n",
    "\n",
    "# Load datasets\n",
    "assessments = pd.read_csv(assessments_path)\n",
    "courses = pd.read_csv(courses_path)\n",
    "student_assessment = pd.read_csv(student_assessment_path)\n",
    "student_info = pd.read_csv(student_info_path)\n",
    "student_registration = pd.read_csv(student_registration_path)\n",
    "student_vle = pd.read_csv(student_vle_path)\n",
    "vle = pd.read_csv(vle_path)\n",
    "\n",
    "# Verify the first few rows of each dataset\n",
    "print(\"Assessments Data:\", assessments.head())\n",
    "print(\"Courses Data:\", courses.head())\n",
    "print(\"Student Assessment Data:\", student_assessment.head())\n",
    "print(\"Student Info Data:\", student_info.head())\n",
    "print(\"Student Registration Data:\", student_registration.head())\n",
    "print(\"Student VLE Data:\", student_vle.head())\n",
    "print(\"VLE Data:\", vle.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in Assessments: ['code_module', 'code_presentation', 'id_assessment', 'assessment_type', 'date', 'weight']\n",
      "Columns in Courses: ['code_module', 'code_presentation', 'module_presentation_length']\n",
      "Columns in Student Assessment: ['id_assessment', 'id_student', 'date_submitted', 'is_banked', 'score']\n",
      "Columns in Student Info: ['code_module', 'code_presentation', 'id_student', 'gender', 'region', 'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts', 'studied_credits', 'disability', 'final_result']\n",
      "Columns in Student Registration: ['code_module', 'code_presentation', 'id_student', 'date_registration', 'date_unregistration']\n",
      "Columns in Student VLE: ['code_module', 'code_presentation', 'id_student', 'id_site', 'date', 'sum_click']\n",
      "Columns in VLE: ['id_site', 'code_module', 'code_presentation', 'activity_type', 'week_from', 'week_to']\n"
     ]
    }
   ],
   "source": [
    "# Print the column names of each DataFrame to ensure consistency and correct references in later processing\n",
    "print(\"Columns in Assessments:\", assessments.columns.tolist())\n",
    "print(\"Columns in Courses:\", courses.columns.tolist())\n",
    "print(\"Columns in Student Assessment:\", student_assessment.columns.tolist())\n",
    "print(\"Columns in Student Info:\", student_info.columns.tolist())\n",
    "print(\"Columns in Student Registration:\", student_registration.columns.tolist())\n",
    "print(\"Columns in Student VLE:\", student_vle.columns.tolist())\n",
    "print(\"Columns in VLE:\", vle.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and clean data\n",
    "def preprocess_data(student_info):\n",
    "    # Convert id_student to string and encode the final_result\n",
    "    student_info['id_student'] = student_info['id_student'].astype(str)\n",
    "    encoder = LabelEncoder()\n",
    "    student_info['final_result'] = encoder.fit_transform(student_info['final_result'])\n",
    "    return student_info\n",
    "\n",
    "# Call the preprocess function\n",
    "student_info = preprocess_data(student_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courses columns: ['code_module', 'code_presentation', 'module_presentation_length']\n",
      "Student VLE columns: ['code_module', 'code_presentation', 'id_student', 'id_site', 'date', 'sum_click']\n",
      "'code_module' in Courses: True\n",
      "'code_module' in Student VLE: True\n"
     ]
    }
   ],
   "source": [
    "# Print columns again to double-check\n",
    "print(\"Courses columns:\", courses.columns.tolist())\n",
    "print(\"Student VLE columns:\", student_vle.columns.tolist())\n",
    "\n",
    "# Check specifically for 'code_module' in both DataFrames\n",
    "print(\"'code_module' in Courses:\", 'code_module' in courses.columns.tolist())\n",
    "print(\"'code_module' in Student VLE:\", 'code_module' in student_vle.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data types before merge:\n",
      "Student VLE 'code_module' type: object\n",
      "Courses 'code_module' type: object\n",
      "Student VLE 'code_presentation' type: object\n",
      "Courses 'code_presentation' type: object\n",
      "Aggregated Daily Clicks:\n",
      "   id_student  date code_module code_presentation  sum_click\n",
      "0        6516   -23         AAA             2014J         28\n",
      "1        6516   -22         AAA             2014J         82\n",
      "2        6516   -20         AAA             2014J         41\n",
      "3        6516   -17         AAA             2014J          7\n",
      "4        6516   -12         AAA             2014J          2\n",
      "Merged Daily Clicks with Courses:\n",
      "   id_student  date code_module code_presentation  sum_click  \\\n",
      "0        6516   -23         AAA             2014J         28   \n",
      "1        6516   -22         AAA             2014J         82   \n",
      "2        6516   -20         AAA             2014J         41   \n",
      "3        6516   -17         AAA             2014J          7   \n",
      "4        6516   -12         AAA             2014J          2   \n",
      "\n",
      "   module_presentation_length  \n",
      "0                         269  \n",
      "1                         269  \n",
      "2                         269  \n",
      "3                         269  \n",
      "4                         269  \n"
     ]
    }
   ],
   "source": [
    "def prepare_time_series_features(student_vle, courses):\n",
    "    print(\"Checking data types before merge:\")\n",
    "    print(\"Student VLE 'code_module' type:\", student_vle['code_module'].dtype)\n",
    "    print(\"Courses 'code_module' type:\", courses['code_module'].dtype)\n",
    "    print(\"Student VLE 'code_presentation' type:\", student_vle['code_presentation'].dtype)\n",
    "    print(\"Courses 'code_presentation' type:\", courses['code_presentation'].dtype)\n",
    "\n",
    "    # Ensure data types match\n",
    "    student_vle['code_module'] = student_vle['code_module'].astype(str)\n",
    "    courses['code_module'] = courses['code_module'].astype(str)\n",
    "    student_vle['code_presentation'] = student_vle['code_presentation'].astype(str)\n",
    "    courses['code_presentation'] = courses['code_presentation'].astype(str)\n",
    "    \n",
    "    # Aggregate clicks by day, student, and course info\n",
    "    daily_clicks = student_vle.groupby(['id_student', 'date', 'code_module', 'code_presentation']).agg({'sum_click': 'sum'}).reset_index()\n",
    "    \n",
    "    print(\"Aggregated Daily Clicks:\")\n",
    "    print(daily_clicks.head())  # Show the first few rows to verify\n",
    "    \n",
    "    # Merge with courses to include the module presentation length\n",
    "    daily_clicks = pd.merge(daily_clicks, courses[['code_module', 'code_presentation', 'module_presentation_length']],\n",
    "                            on=['code_module', 'code_presentation'], how='left')\n",
    "\n",
    "    print(\"Merged Daily Clicks with Courses:\")\n",
    "    print(daily_clicks.head())  # Verify the merge was successful\n",
    "\n",
    "    # Normalize the date values by the length of the course presentation\n",
    "    daily_clicks['date'] = daily_clicks['date'] / daily_clicks['module_presentation_length']\n",
    "    \n",
    "    return daily_clicks\n",
    "\n",
    "# Prepare daily clicks data\n",
    "daily_clicks = prepare_time_series_features(student_vle, courses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Clicks with Normalized Date:\n",
      "   id_student      date code_module code_presentation  sum_click  \\\n",
      "0        6516 -0.085502         AAA             2014J         28   \n",
      "1        6516 -0.081784         AAA             2014J         82   \n",
      "2        6516 -0.074349         AAA             2014J         41   \n",
      "3        6516 -0.063197         AAA             2014J          7   \n",
      "4        6516 -0.044610         AAA             2014J          2   \n",
      "\n",
      "   normalized_date  \n",
      "0        -0.000318  \n",
      "1        -0.000304  \n",
      "2        -0.000276  \n",
      "3        -0.000235  \n",
      "4        -0.000166  \n"
     ]
    }
   ],
   "source": [
    "# Normalize the date values and verify the resulting data\n",
    "daily_clicks['normalized_date'] = daily_clicks['date'] / daily_clicks['module_presentation_length']\n",
    "\n",
    "# Drop unnecessary columns if needed (e.g., module_presentation_length if no longer required)\n",
    "daily_clicks = daily_clicks.drop(columns=['module_presentation_length'])\n",
    "\n",
    "# Verify the normalized dataset\n",
    "print(\"Daily Clicks with Normalized Date:\")\n",
    "print(daily_clicks.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched Data:\n",
      "  id_student      date code_module code_presentation  sum_click  \\\n",
      "0       6516 -0.085502         AAA             2014J         28   \n",
      "1       6516 -0.081784         AAA             2014J         82   \n",
      "2       6516 -0.074349         AAA             2014J         41   \n",
      "3       6516 -0.063197         AAA             2014J          7   \n",
      "4       6516 -0.044610         AAA             2014J          2   \n",
      "\n",
      "   normalized_date gender    region highest_education imd_band age_band  \\\n",
      "0        -0.000318      M  Scotland  HE Qualification   80-90%     55<=   \n",
      "1        -0.000304      M  Scotland  HE Qualification   80-90%     55<=   \n",
      "2        -0.000276      M  Scotland  HE Qualification   80-90%     55<=   \n",
      "3        -0.000235      M  Scotland  HE Qualification   80-90%     55<=   \n",
      "4        -0.000166      M  Scotland  HE Qualification   80-90%     55<=   \n",
      "\n",
      "   num_of_prev_attempts  studied_credits disability  final_result  \n",
      "0                     0               60          N             2  \n",
      "1                     0               60          N             2  \n",
      "2                     0               60          N             2  \n",
      "3                     0               60          N             2  \n",
      "4                     0               60          N             2  \n"
     ]
    }
   ],
   "source": [
    "# Ensure `id_student` has the same data type in both DataFrames\n",
    "daily_clicks['id_student'] = daily_clicks['id_student'].astype(str)\n",
    "student_info['id_student'] = student_info['id_student'].astype(str)\n",
    "\n",
    "# Merge daily_clicks with student_info to enrich the data\n",
    "enriched_data = pd.merge(\n",
    "    daily_clicks,\n",
    "    student_info[['id_student', 'code_module', 'code_presentation', 'gender', 'region',\n",
    "                  'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts',\n",
    "                  'studied_credits', 'disability', 'final_result']],\n",
    "    on=['id_student', 'code_module', 'code_presentation'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verify the enriched data\n",
    "print(\"Enriched Data:\")\n",
    "print(enriched_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the graph with optimized features...\n",
      "Graph built successfully!\n",
      "Graph has 26074 nodes and 3616194 edges.\n",
      "Node features shape: torch.Size([26074, 36])\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def build_graph_optimized_with_correct_features(enriched_data):\n",
    "    print(\"Building the graph with optimized features...\")\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    categorical_columns = ['gender', 'region', 'highest_education', 'imd_band', 'age_band']\n",
    "    encoder = OneHotEncoder(sparse_output=False)  # Use `sparse_output` instead of `sparse`\n",
    "    encoded_categorical_features = encoder.fit_transform(enriched_data[categorical_columns])\n",
    "\n",
    "    # Combine numerical and encoded categorical features\n",
    "    numerical_features = enriched_data[['studied_credits', 'num_of_prev_attempts']].values\n",
    "    enriched_data['combined_features'] = list(np.hstack([numerical_features, encoded_categorical_features]))\n",
    "\n",
    "    # Aggregate features by unique student nodes\n",
    "    grouped_features = enriched_data.groupby('id_student')['combined_features'].first()\n",
    "    all_features = torch.tensor(np.vstack(grouped_features.values), dtype=torch.float32)\n",
    "\n",
    "    # Get unique student IDs for nodes\n",
    "    unique_students = enriched_data['id_student'].unique()\n",
    "    student_mapping = {student: idx for idx, student in enumerate(unique_students)}\n",
    "\n",
    "    # Map students to node IDs\n",
    "    enriched_data['student_node'] = enriched_data['id_student'].map(student_mapping)\n",
    "\n",
    "    # Create edges based on the same course and presentation (one edge per group)\n",
    "    edges_src = []\n",
    "    edges_dst = []\n",
    "    grouped = enriched_data.groupby(['code_module', 'code_presentation'])\n",
    "    for _, group in grouped:\n",
    "        students = group['student_node'].tolist()\n",
    "        # Create edges by connecting each student to the next (linear connections)\n",
    "        for i in range(len(students) - 1):\n",
    "            edges_src.append(students[i])\n",
    "            edges_dst.append(students[i + 1])\n",
    "            edges_src.append(students[i + 1])  # Reverse edge for undirected graph\n",
    "            edges_dst.append(students[i])\n",
    "\n",
    "    # Create the graph\n",
    "    graph = dgl.graph((edges_src, edges_dst))\n",
    "\n",
    "    # Add features to the graph\n",
    "    graph.ndata['features'] = all_features\n",
    "\n",
    "    print(\"Graph built successfully!\")\n",
    "    print(f\"Graph has {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(f\"Node features shape: {graph.ndata['features'].shape}\")\n",
    "\n",
    "    return graph, student_mapping\n",
    "\n",
    "# Call the function to build the graph\n",
    "graph, student_mapping = build_graph_optimized_with_correct_features(enriched_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Information:\n",
      "Graph(num_nodes=26074, num_edges=3616194,\n",
      "      ndata_schemes={'features': Scheme(shape=(36,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "Node features shape: torch.Size([26074, 36])\n",
      "Number of edges: 3616194\n"
     ]
    }
   ],
   "source": [
    "# Print basic graph information\n",
    "print(\"Graph Information:\")\n",
    "print(graph)\n",
    "\n",
    "# Verify node and edge features\n",
    "print(\"Node features shape:\", graph.ndata['features'].shape)\n",
    "print(\"Number of edges:\", graph.number_of_edges())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import GraphConv\n",
    "import dgl\n",
    "\n",
    "\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, num_layers):\n",
    "        super(GNNModel, self).__init__()\n",
    "        \n",
    "        # Linear layer to project input features to hidden_dim\n",
    "        self.input_proj = nn.Linear(in_feats, hidden_feats)\n",
    "\n",
    "        # GraphConv layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(GraphConv(hidden_feats, hidden_feats, activation=F.relu))\n",
    "\n",
    "        # Output layer for node-level classification\n",
    "        self.output_layer = nn.Linear(hidden_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        # Ensure the graph has self-loops\n",
    "        g = add_self_loops_if_needed(g)\n",
    "\n",
    "        # Project input features to hidden_dim\n",
    "        h = self.input_proj(features)\n",
    "\n",
    "        # Pass through GNN layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(g, h)\n",
    "\n",
    "        # Output for each node\n",
    "        out = self.output_layer(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, g, features, labels, epochs, optimizer, loss_fn):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(g, features)\n",
    "       \n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        train_acc = (logits.argmax(dim=1) == labels).float().mean().item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, g, features, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "        predicted = logits.argmax(dim=1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / labels.size(0)\n",
    "        print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paramètres dans le modèle : 15108\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Nombre de paramètres dans le modèle : {num_params}\")\n",
    "# Assurez-vous que les labels sont alignés avec les nœuds\n",
    "labels = enriched_data.groupby('id_student')['final_result'].first().values\n",
    "labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "# Vérifiez la correspondance\n",
    "assert graph.number_of_nodes() == labels.shape[0], \"Mismatch between nodes and labels!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature shape: torch.Size([26074, 36])\n",
      "Label shape: torch.Size([26074])\n"
     ]
    }
   ],
   "source": [
    "# Vérification des caractéristiques des nœuds\n",
    "print(\"Node feature shape:\", graph.ndata['features'].shape)\n",
    "\n",
    "# Vérification des labels\n",
    "print(\"Label shape:\", labels.shape)\n",
    "\n",
    "# Assurez-vous que le nombre de nœuds correspond au nombre de labels\n",
    "assert graph.number_of_nodes() == labels.shape[0], \"Mismatch between nodes and labels!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamètres\n",
    "in_feats = graph.ndata['features'].shape[1]\n",
    "hidden_feats = 64\n",
    "out_feats = len(enriched_data['final_result'].unique())\n",
    "num_layers = 3\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Modèle\n",
    "model = GNNModel(in_feats, hidden_feats, out_feats, num_layers).to(device)\n",
    "\n",
    "# Optimiseur et fonction de perte\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.2598, Train Acc: 0.4366\n",
      "Epoch 2/50, Loss: 1.2590, Train Acc: 0.4382\n",
      "Epoch 3/50, Loss: 1.2588, Train Acc: 0.4382\n",
      "Epoch 4/50, Loss: 1.2591, Train Acc: 0.4374\n",
      "Epoch 5/50, Loss: 1.2586, Train Acc: 0.4381\n",
      "Epoch 6/50, Loss: 1.2582, Train Acc: 0.4382\n",
      "Epoch 7/50, Loss: 1.2583, Train Acc: 0.4378\n",
      "Epoch 8/50, Loss: 1.2580, Train Acc: 0.4383\n",
      "Epoch 9/50, Loss: 1.2577, Train Acc: 0.4377\n",
      "Epoch 10/50, Loss: 1.2576, Train Acc: 0.4379\n",
      "Epoch 11/50, Loss: 1.2575, Train Acc: 0.4385\n",
      "Epoch 12/50, Loss: 1.2571, Train Acc: 0.4383\n",
      "Epoch 13/50, Loss: 1.2569, Train Acc: 0.4384\n",
      "Epoch 14/50, Loss: 1.2568, Train Acc: 0.4386\n",
      "Epoch 15/50, Loss: 1.2566, Train Acc: 0.4388\n",
      "Epoch 16/50, Loss: 1.2563, Train Acc: 0.4385\n",
      "Epoch 17/50, Loss: 1.2561, Train Acc: 0.4386\n",
      "Epoch 18/50, Loss: 1.2560, Train Acc: 0.4392\n",
      "Epoch 19/50, Loss: 1.2557, Train Acc: 0.4389\n",
      "Epoch 20/50, Loss: 1.2554, Train Acc: 0.4396\n",
      "Epoch 21/50, Loss: 1.2550, Train Acc: 0.4389\n",
      "Epoch 22/50, Loss: 1.2548, Train Acc: 0.4388\n",
      "Epoch 23/50, Loss: 1.2546, Train Acc: 0.4398\n",
      "Epoch 24/50, Loss: 1.2543, Train Acc: 0.4396\n",
      "Epoch 25/50, Loss: 1.2539, Train Acc: 0.4403\n",
      "Epoch 26/50, Loss: 1.2536, Train Acc: 0.4398\n",
      "Epoch 27/50, Loss: 1.2533, Train Acc: 0.4399\n",
      "Epoch 28/50, Loss: 1.2530, Train Acc: 0.4408\n",
      "Epoch 29/50, Loss: 1.2526, Train Acc: 0.4399\n",
      "Epoch 30/50, Loss: 1.2523, Train Acc: 0.4411\n",
      "Epoch 31/50, Loss: 1.2518, Train Acc: 0.4403\n",
      "Epoch 32/50, Loss: 1.2514, Train Acc: 0.4404\n",
      "Epoch 33/50, Loss: 1.2511, Train Acc: 0.4411\n",
      "Epoch 34/50, Loss: 1.2506, Train Acc: 0.4408\n",
      "Epoch 35/50, Loss: 1.2502, Train Acc: 0.4420\n",
      "Epoch 36/50, Loss: 1.2496, Train Acc: 0.4409\n",
      "Epoch 37/50, Loss: 1.2491, Train Acc: 0.4420\n",
      "Epoch 38/50, Loss: 1.2486, Train Acc: 0.4413\n",
      "Epoch 39/50, Loss: 1.2481, Train Acc: 0.4414\n",
      "Epoch 40/50, Loss: 1.2476, Train Acc: 0.4420\n",
      "Epoch 41/50, Loss: 1.2471, Train Acc: 0.4417\n",
      "Epoch 42/50, Loss: 1.2466, Train Acc: 0.4432\n",
      "Epoch 43/50, Loss: 1.2461, Train Acc: 0.4422\n",
      "Epoch 44/50, Loss: 1.2456, Train Acc: 0.4430\n",
      "Epoch 45/50, Loss: 1.2451, Train Acc: 0.4425\n",
      "Epoch 46/50, Loss: 1.2446, Train Acc: 0.4426\n",
      "Epoch 47/50, Loss: 1.2441, Train Acc: 0.4417\n",
      "Epoch 48/50, Loss: 1.2436, Train Acc: 0.4430\n",
      "Epoch 49/50, Loss: 1.2431, Train Acc: 0.4418\n",
      "Epoch 50/50, Loss: 1.2427, Train Acc: 0.4435\n",
      "Evaluation Accuracy: 0.4420\n"
     ]
    }
   ],
   "source": [
    "# Entraînement\n",
    "train_model(model, graph, graph.ndata['features'], labels, epochs, optimizer, loss_fn)\n",
    "\n",
    "# Évaluation\n",
    "evaluate_model(model, graph, graph.ndata['features'], labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
